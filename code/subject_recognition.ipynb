{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: You are using GPU for training\n",
      "<class 'list'>\n",
      "load train data, batch_num: 566\tbatch_size: 128\n",
      "<class 'list'>\n",
      "load valid data, batch_num: 82\tbatch_size: 128\n",
      "load word vocab, size: 61024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "from args import get_args\n",
    "from models import RNNSubjectRecognition\n",
    "# from DataLoader import SubjectRecognitionLoader, load_pretrained_vectors\n",
    "from predict import evaluation\n",
    "sys.path.append(\"/home2/zyk/kbqa/entity_detection\")\n",
    "from seqLabelingLoader import SeqLabelingLoader\n",
    "\n",
    "def set_logger(name):\n",
    "    '''\n",
    "    Write logs to checkpoint and console\n",
    "    '''\n",
    "\n",
    "    log_file = './log/%s.log' % name\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "        level=logging.INFO,\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        filename=log_file,\n",
    "        filemode='w'\n",
    "    )\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "    \n",
    "class ARGS():\n",
    "    def __init__(self):\n",
    "        self.cuda = True\n",
    "        self.gpu = 3\n",
    "        self.seed = 1111\n",
    "        self.train_file = '/home2/zyk/kbqa/entity_detection/data/train.entity_detection.pt'\n",
    "        self.valid_file = '/home2/zyk/kbqa/entity_detection/data/valid.entity_detection.pt'\n",
    "        self.test_file = '/home2/zyk/kbqa/entity_detection/data/test.entity_detection.pt'\n",
    "        self.vocab_file = '../../data/vocab/word_vocab.pt'\n",
    "        self.birnn = True\n",
    "        self.rnn_type = 'lstm'\n",
    "        self.d_embed=300\n",
    "        self.d_hidden=200\n",
    "        self.n_layers=2\n",
    "        self.lr=1e-4\n",
    "        self.dropout_prob=0.5\n",
    "\n",
    "        self.word_vectors='../../data/vocab/glove.42B.300d.txt'\n",
    "        self.vector_cache='./input_vectors.pt'\n",
    "        self.word_normalize=True\n",
    "\n",
    "        \n",
    "args = ARGS()\n",
    "torch.manual_seed(args.seed)\n",
    "if not args.cuda:\n",
    "    args.gpu = -1\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:%d\" % args.gpu)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    print(\"Note: You are using GPU for training\")\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"Warning: You have Cuda but do not use it. You are using CPU for training\")\n",
    "\n",
    "train_loader = SeqLabelingLoader(args.train_file, args.gpu)\n",
    "print('load train data, batch_num: %d\\tbatch_size: %d'\n",
    "      %(train_loader.batch_num, train_loader.batch_size))\n",
    "valid_loader = SeqLabelingLoader(args.valid_file, args.gpu)\n",
    "print('load valid data, batch_num: %d\\tbatch_size: %d'\n",
    "      %(valid_loader.batch_num, valid_loader.batch_size))\n",
    "\n",
    "# load word vocab for questions\n",
    "word_vocab = torch.load(args.vocab_file)\n",
    "print('load word vocab, size: %s' % len(word_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight torch.Size([61024, 300])\n",
      "rnn.weight_ih_l0 torch.Size([800, 300])\n",
      "rnn.weight_hh_l0 torch.Size([800, 200])\n",
      "rnn.bias_ih_l0 torch.Size([800])\n",
      "rnn.bias_hh_l0 torch.Size([800])\n",
      "rnn.weight_ih_l0_reverse torch.Size([800, 300])\n",
      "rnn.weight_hh_l0_reverse torch.Size([800, 200])\n",
      "rnn.bias_ih_l0_reverse torch.Size([800])\n",
      "rnn.bias_hh_l0_reverse torch.Size([800])\n",
      "rnn.weight_ih_l1 torch.Size([800, 400])\n",
      "rnn.weight_hh_l1 torch.Size([800, 200])\n",
      "rnn.bias_ih_l1 torch.Size([800])\n",
      "rnn.bias_hh_l1 torch.Size([800])\n",
      "rnn.weight_ih_l1_reverse torch.Size([800, 400])\n",
      "rnn.weight_hh_l1_reverse torch.Size([800, 200])\n",
      "rnn.bias_ih_l1_reverse torch.Size([800])\n",
      "rnn.bias_hh_l1_reverse torch.Size([800])\n",
      "hidden2tag.0.weight torch.Size([400, 400])\n",
      "hidden2tag.0.bias torch.Size([400])\n",
      "hidden2tag.1.weight torch.Size([400])\n",
      "hidden2tag.1.bias torch.Size([400])\n",
      "hidden2tag.4.weight torch.Size([2, 400])\n",
      "hidden2tag.4.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../tools')\n",
    "args.n_out = 2\n",
    "args.n_cells = args.n_layers\n",
    "if args.birnn:\n",
    "    args.n_cells *= 2\n",
    "\n",
    "model = SubjectRecognition(len(word_vocab), args)\n",
    "if args.word_vectors:\n",
    "    if os.path.isfile(args.vector_cache):\n",
    "        pretrained = torch.load(args.vector_cache)\n",
    "    else:\n",
    "        pretrained = load_pretrained_vectors(args.word_vectors, binary=False,\n",
    "                                             normalize=args.word_normalize)\n",
    "        torch.save(pretrained, args.vector_cache)\n",
    "\n",
    "    model.embed.weight.data.copy_(pretrained)\n",
    "    logging.info('load pretrained word vectors from %s, pretrained size: %s' % (args.word_vectors,\n",
    "                                                                         pretrained.size()))\n",
    "\n",
    "model.to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "criterion = nn.NLLLoss() # negative log likelyhood loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/zyk/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home2/zyk/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 900\n",
      "tensor(7, device='cuda:3')\n",
      "0.3364485981308411\n",
      "0.28125\n",
      "0.30638297872340425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/zyk/anaconda3/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type SubjectRecognition. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'log_softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-830dd260754f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c63fc8cb95ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, input_length)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# shape of `outputs` - (sequence length, batch size, hidden size X num directions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_resorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'log_softmax'"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "iterations = 0\n",
    "best_dev_acc = 0\n",
    "best_dev_F = 0\n",
    "num_iters_in_epoch = train_loader.batch_num\n",
    "patience = 5 * num_iters_in_epoch\n",
    "iters_not_improved = 0\n",
    "early_stop = False\n",
    "\n",
    "snapshot_path = \"subject_recognition.pth\"\n",
    "best_snapshot_path = \"best_subject_recognition.pth\"\n",
    "\n",
    "for epoch in range(1, 10+1):\n",
    "    if early_stop:\n",
    "        logging.info(\"Early stopping. Epoch: {}, Best Dev. Acc: {}\".format(epoch, best_dev_acc))\n",
    "        break\n",
    "\n",
    "    n_correct, n_total = 0, 0\n",
    "    for batch_idx, batch in enumerate(train_loader.next_batch()):\n",
    "        iterations += 1\n",
    "        seq, label = batch\n",
    "        seq_len = [seq.size()[1] for i in range(seq.size()[0]) ]\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        scores = model.forward(seq, seq_len)\n",
    "\n",
    "        n_correct += ((torch.max(scores, dim=1)[1].view(label.size()).data == label.data).sum(dim=0) \\\n",
    "                      == label.size()[0]).sum()\n",
    "        n_total += train_loader.batch_size\n",
    "        train_acc = 100. * n_correct / n_total\n",
    "\n",
    "        loss = criterion(scores, label.view(-1, 1)[:, 0])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.6)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iterations % 1000 == 0:\n",
    "            torch.save(model, snapshot_path)\n",
    "\n",
    "        if iterations % 900 == 0:\n",
    "            model.eval()\n",
    "            n_dev_correct = 0\n",
    "            gold_list = []\n",
    "            pred_list = []\n",
    "            for valid_batch_idx, valid_batch in enumerate(valid_loader.next_batch()):\n",
    "                valid_label = valid_batch[1]\n",
    "                seq, valid_label = batch\n",
    "                seq_len = [seq.size()[1] for i in range(seq.size()[0]) ]\n",
    "                answer = model.forward(seq, seq_len)\n",
    "                n_dev_correct += ((torch.max(answer, 1)[1].view(valid_label.size()).data == \\\n",
    "                                   valid_label.data).sum(dim=0) == valid_label.size()[0]).sum()\n",
    "                index_tag = np.transpose(torch.max(answer, 1)[1].view(valid_label.size()).cpu().data.numpy())\n",
    "                gold_list.append(np.transpose(valid_label.cpu().data.numpy()))\n",
    "                pred_list.append(index_tag)\n",
    "            P, R, F_ = evaluation(gold_list, pred_list)\n",
    "            dev_acc = 100. * n_dev_correct / (valid_loader.batch_num * valid_loader.batch_size)\n",
    "            print(\"iterations: %d\" % iterations)\n",
    "            print(dev_acc)\n",
    "            print(P)\n",
    "            print(R)\n",
    "            print(F)\n",
    "            if F_ > best_dev_F:\n",
    "                best_dev_F = F_\n",
    "                iters_not_improved = 0\n",
    "                torch.save(model, best_snapshot_path)\n",
    "            else:\n",
    "                iters_not_improved += 1\n",
    "                if iters_not_improved > patience:\n",
    "                    early_stop = True\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
